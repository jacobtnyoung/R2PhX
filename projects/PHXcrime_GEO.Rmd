---
title: "Geocoding the Phoenix Crime Data"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    theme: yeti
    toc: false
    toc_float: false
    collapsed: false
    smooth_scroll: false
---

```{r setup, include=FALSE}

knitr::opts_chunk$set( echo = TRUE, 
                       message = FALSE, 
                       warning = FALSE, 
                       fig.width = 12, 
                       fig.height = 10 )

rm( list = ls() )

```

# Overview

This codethrough shows how to link crimes reported to the police in Phoenix to a geographic area. That way, the spatial distribution of these incidents can be examined.

<br>

## Challenge

The data are crimes reported to the police. The challenge is that the only spatial information in the data are addresses and zip codes. But, the addresses are scrubbed of the last two digits. For example, an address may appear as 15XX W. Lavender Rd. This could be 1501 or 1548.

But, it turns out that we can still pull the geographic data we need. Let's check it out.

<br>

## Step 1: Link the tracts to the crime data

<br>

### Start by getting the crime data

<br>

```{r}

# libraries we will need
library( dplyr )
library( pander )

# Get the data from the open data potal
url <- "https://www.phoenixopendata.com/dataset/cc08aace-9ca9-467f-b6c1-f0879ab1a358/resource/0ce3411a-2fc6-4302-a33f-167f68608a20/download/crimestat.csv"

crime.data <- read.csv( url, as.is = TRUE, header = TRUE )

# drop missing cases
crime.data <- na.omit( crime.data )

# take a look at the first few cases
head( crime.data ) %>% pander()

# take a look at the addresses
# we can see how they are redacted
head( crime.data$X100.BLOCK.ADDR )

```

<br>

### Work with the addresses

To overcome this issue we first use the `tidygeocoder` package. This package has a function called `geo()` that can return a variety of information about an address. There is wide functionality for this function, so if you are interested in using it, be sure to read through the help package (i.e. `?geo`). So, we will first use the `geo()` function to get the spatial information about the incidents. Specifically, the latitude and longitude.

<br>

```{r}

# Libraries we will need
library( tidyr )
library( tidygeocoder )
library( tidycensus )

# set the api key
census_api_key( "8f1ce150e65b8cba01951fbcbbe65ebbb9409638" )

# fill these two digits with 50 and then append the zip code
# we use 50 since that is midway between the street segment
crime.data$Address.adj <- gsub( "XX", "50", crime.data$X100.BLOCK.ADDR )

# append the zip code
crime.data$Address.zip <- paste( crime.data$Address.adj, crime.data$ZIP )

# sample 100 cases for demonstration
set.seed( 54321 )
crime.data.sub <- crime.data[sample( nrow( crime.data ), 100, replace = FALSE ), ]

# pull the spatial geographic data
spatial <- geo( 
  crime.data.sub$Address.zip,  # the addresses with zip codes
  full_results = TRUE,         # we want all of the data from the geocoding service
  method = "census",           # we want the information from the census
  api_options = list( census_return_type = "geographies" )
  )

# examine our object
head( spatial )

# look at how many matches we have
table( is.na( spatial$census_block ) )

```

<br>

According to the table, `r table( is.na( spatial$census_block ) )[2]` cases are missing spatial data. That means, we were not able to link those addresses to the information in the census API. These cases will be dropped below.

<br>

Now, let's link the spatial data back to the crime data.

<br>

```{r}

# add the incident number to the spatial data
spatial$INC.NUMBER <- crime.data.sub$INC.NUMBER

# append the spatial data
crime.data.sp.sub <- merge( crime.data.sub, spatial, 
                         by.x="INC.NUMBER", by.y="INC.NUMBER")

# remove those that are missing spacial data
crime.data.sp.sub <- 
  crime.data.sp.sub[is.na( crime.data.sp.sub$census_block ) == FALSE, ]

# create a count of crimes per tract
counts <- crime.data.sp.sub %>% 
  select( UCR.CRIME.CATEGORY, census_tract ) %>% 
  group_by( census_tract ) %>% 
  summarize( counts = n() )

# add the counts to the object
crime.data.sp.sub <- merge( crime.data.sp.sub, counts,
                             by.x = "census_tract", by.y = "census_tract" )
  
```

<br>

## Step 2: Plotting the data

Now that we have the spatial data, we can use the latitude and longitude to map the data.

<br>

### Build a simple plot for Phoenix

<br>

```{r, results = "hide"}

# use the tigris package to get the tract info
library( tigris )

# get the tracts
phx <- tracts( state="AZ", county="Maricopa", cb=TRUE )

# add the subsample of crimes
phx <- merge( phx, crime.data.sp.sub, by.x="TRACTCE", by.y="census_tract" )

# plot it
library( ggplot2 )

phx %>% 
  ggplot() + 
  geom_sf( aes(),color="aquamarine4" ) +
  geom_point( aes( x = long, y = lat ), color = "darkblue" ) +
  theme_void() + 
  theme( panel.grid.major = element_line( colour = 'transparent' ) ) +
  scale_fill_distiller( palette="Reds", direction=1, name="Estimate" ) +
  labs( title="Example of Plotting \nCrimes in Phoenix\n (points are crimes)", caption="Source: City of Phoenix \nOpen Data Portal" ) 

```

<br>

### Build a better plot for Phoenix

The plot above is ok, but we can do better. Using the `qmplot()` function in the `ggmap` library is an option.

<br>

```{r, warning=FALSE}

library( ggmap )

qmplot( long, lat, data = as.data.frame( phx ), zoom = 11 ) +
  ggtitle( "Example of Plotting \nCrimes in Phoenix\n (points are crimes)" )
  
```  

<br>

## Step 3: Executing the steps for a larger set of data

Above we used the census API for our coordinates. A limitation is that this API only allows batches of 10,000. Since our data file has `r dim( crime.data )[1]` cases, we cannot do this in a single call.  

A further complication is that a call to the API takes time. For example, let's look at how long our subset used above took:

<br>

```{r}

start_time <- Sys.time()

spatial <- geo( crime.data.sub$Address.zip, full_results = TRUE, method = "census", api_options = list( census_return_type = "geographies" ) )

end_time <- Sys.time()

end_time - start_time

```

That is an elapsed time of nearly 6 seconds for 100 cases. If we are to pass 10,000 cases to the API, that would take `r round( ( 6*10000 ) / 60, 0 )` minutes. That is `r round( round( ( 6*10000 ) / 60, 1 ) / 60, 0 )` hours! Given that we have `r dim(crime.data)[1]` cases, this would require a substantial amount of time!

<br>

For example, let's rerun our code above, but with 5000 cases, rather than 100.

<br>

```{r, cache=TRUE}

# sample 100 cases for demonstration
set.seed( 54321 )
crime.data.sub2 <- 
  crime.data[sample( nrow( crime.data ), 5000, replace = FALSE ), ]

# start the timer
start_time <- Sys.time()

# pull the spatial geographic data
spatial2 <- geo( 
  crime.data.sub2$Address.zip,  # the addresses with zip codes
  full_results = TRUE,         # we want all of the data from the geocoding service
  method = "census",           # we want the information from the census
  api_options = list( census_return_type = "geographies" )
  )

# how long did it take?
end_time <- Sys.time()

# add the incident number to the spatial data
spatial2$INC.NUMBER <- crime.data.sub2$INC.NUMBER

# append the spatial data
crime.data.sp.sub2 <- merge( crime.data.sub2, spatial2, 
                            by.x="INC.NUMBER", by.y="INC.NUMBER")

# remove those that are missing spacial data
crime.data.sp.sub2 <- 
  crime.data.sp.sub2[is.na( crime.data.sp.sub2$census_block ) == FALSE, ]

# create a count of crimes per tract
counts2 <- crime.data.sp.sub2 %>% 
  select( UCR.CRIME.CATEGORY, census_tract ) %>% 
  group_by( census_tract ) %>% 
  summarize( counts = n() )

# add the counts to the object
crime.data.sp.sub2 <- merge( crime.data.sp.sub2, counts2,
                            by.x = "census_tract", by.y = "census_tract" )

# get the tracts
phx2 <- tracts( state="AZ", county="Maricopa", cb=TRUE )

# add the subsample of crimes
phx2 <- merge( phx2, crime.data.sp.sub2, by.x="TRACTCE", by.y="census_tract" )

# plot it
qmplot( long, lat, data = as.data.frame( phx2 ), zoom = 11 ) +
  ggtitle( "Example of Plotting \nCrimes in Phoenix\n (points are crimes)" )

```

<br>

How long did it take?

  + `r round( end_time - start_time, 1 )` minutes

Thus, it took several minutes to pull data for 5000 cases. But, we do get a better plot.

<br>

## Summing up

We were able to link the coordinates to the crime data. This allowed us to plot them on a map.

### Next steps

Some next steps would be to:

  + Sort the data by time, year, crime type, etc.
  
  + Create a batch to run the script over all of the data files. This could be done by creating groups of 10000 cases and running those as separate code chunks. Since only the most recent data will change, we just have to keep the older data cached somewhere and then call it if we want to examine it.
  
  + Linking the crime data with other important variables.

<br>

<p align="center">
[Back to R 2 Phoenix page](https://jacobtnyoung.github.io/R2PhX/)
</p>

<br>

***Please*** report any needed corrections to the [Issues](https://github.com/jacobtnyoung/R2PhX/issues) page. Thanks!

<br><br>


<p align="center">
***Last updated `r format(Sys.time(), '%d %B, %Y')`***
</p>

<br>